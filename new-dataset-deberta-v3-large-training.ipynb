{"cells":[{"cell_type":"markdown","metadata":{},"source":["\n","200 examples in the public dataset leaves very little room for training!\n","\n","Using `gpt-3.5-turbo` I created another 500 high quality examples at my expense [that I share freely here](https://www.kaggle.com/datasets/radek1/additional-train-data-for-llm-science-exam). They are what (as of now) pushes this notebook to the highest achieved score across the public notebooks (`0.723`)!\n","\n","If you find the additional training examples useful, please upvote the dataset! ğŸ˜Š\n","\n","ğŸ‘‰ [additional train data for LLM Science Exam ğŸ¥³](https://www.kaggle.com/datasets/radek1/additional-train-data-for-llm-science-exam)\n","\n","Thank you! Appreciate your help! ğŸ™ğŸ™ğŸ™\n","\n","This notebook builds on a [notebook](https://www.kaggle.com/code/leonidkulyk/lb-0-709-llm-se-deberta-v3-large-i-1k-wiki) by LEONID KULYK. Among some of the changes are:\n","\n","* use of a new high quality dataset for training\n","* modified training procedure carried out in the notebook\n","* general streamlining of code/training for readability\n","\n","**A couple of related resources you might find useful:**\n","\n","* [ğŸ“Š 15k high-quality train examples ğŸ†ğŸ”¥ğŸš€](https://www.kaggle.com/datasets/radek1/15k-high-quality-examples) - another 15 000 examples I created to help you grow that train/validation set of yours and improve results\n","* [ğŸ“Š Best Open Source LLM Starter Pack ğŸ§™ğŸš€](https://www.kaggle.com/datasets/radek1/best-llm-starter-pack) -- the largest (and best) open source model I managed to run on Kaggle!\n","* [Science Exam Trained Model Weights ğŸš€](https://www.kaggle.com/datasets/radek1/science-exam-trained-model-weights)"]},{"cell_type":"markdown","metadata":{},"source":["# Preprocessing the dataset"]},{"cell_type":"code","execution_count":152,"metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2023-07-21T12:18:05.52768Z","iopub.status.busy":"2023-07-21T12:18:05.527036Z","iopub.status.idle":"2023-07-21T12:18:12.391158Z","shell.execute_reply":"2023-07-21T12:18:12.389855Z","shell.execute_reply.started":"2023-07-21T12:18:05.527645Z"},"trusted":true},"outputs":[],"source":["from typing import Optional, Union\n","import pandas as pd\n","import numpy as np\n","import torch\n","from datasets import Dataset\n","from dataclasses import dataclass # äºç®€åŒ–åˆ›å»ºå’Œç®¡ç†åŒ…å«æ•°æ®çš„ç±»ã€‚dataclass è£…é¥°å™¨æ˜¯ dataclasses æ¨¡å—çš„ä¸€éƒ¨åˆ†ï¼Œå®ƒå…è®¸ä½ å®šä¹‰ä¸€ä¸ªç±»ï¼Œå…¶ä¸­çš„å±æ€§ï¼ˆæˆå‘˜å˜é‡ï¼‰å¯ä»¥è½»æ¾åœ°è‡ªåŠ¨ç”Ÿæˆåˆå§‹åŒ–æ–¹æ³•ã€\n","from transformers import AutoTokenizer\n","from transformers.tokenization_utils_base import PreTrainedTokenizerBase, PaddingStrategy\n","from transformers import AutoModelForMultipleChoice, TrainingArguments, Trainer, AutoModel\n","\n","deberta_v3_large = '/home/krisfeng/code/llm/kaggle/input/deberta-v3-large-hf-weights'"]},{"cell_type":"code","execution_count":153,"metadata":{},"outputs":[],"source":["torch.cuda.empty_cache()"]},{"cell_type":"markdown","metadata":{},"source":["We begin by loading and processing the train data."]},{"cell_type":"code","execution_count":154,"metadata":{"execution":{"iopub.execute_input":"2023-07-21T12:18:12.393051Z","iopub.status.busy":"2023-07-21T12:18:12.392674Z","iopub.status.idle":"2023-07-21T12:18:12.414514Z","shell.execute_reply":"2023-07-21T12:18:12.413788Z","shell.execute_reply.started":"2023-07-21T12:18:12.393005Z"},"trusted":true},"outputs":[{"data":{"text/plain":["(500, 7)"]},"execution_count":154,"metadata":{},"output_type":"execute_result"}],"source":["df_train = pd.read_csv('./kaggle/input/extra_train_set.csv')\n","#df_train = df_train.drop(columns=\"id\")\n","df_train.shape"]},{"cell_type":"code","execution_count":155,"metadata":{},"outputs":[],"source":["df_valid = pd.read_csv('kaggle/input/train.csv')\n","df_valid = df_valid.drop(columns=\"id\")"]},{"cell_type":"markdown","metadata":{},"source":["Let's add another 500 examples to the train set!"]},{"cell_type":"code","execution_count":156,"metadata":{"execution":{"iopub.execute_input":"2023-07-21T12:18:12.416841Z","iopub.status.busy":"2023-07-21T12:18:12.415834Z","iopub.status.idle":"2023-07-21T12:18:12.435184Z","shell.execute_reply":"2023-07-21T12:18:12.434346Z","shell.execute_reply.started":"2023-07-21T12:18:12.416804Z"},"trusted":true},"outputs":[{"data":{"text/plain":["(26900, 7)"]},"execution_count":156,"metadata":{},"output_type":"execute_result"}],"source":["df_train = pd.concat([\n","    pd.read_csv('./kaggle/input/6000_train_examples.csv'),\n","    pd.read_csv('./kaggle/input/15k_gpt3.5-turbo.csv'),\n","    pd.read_csv('./kaggle/input/5900_examples.csv'),\n","])\n","df_train.reset_index(inplace=True, drop=True)\n","df_train.shape"]},{"cell_type":"code","execution_count":157,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>prompt</th>\n","      <th>A</th>\n","      <th>B</th>\n","      <th>C</th>\n","      <th>D</th>\n","      <th>E</th>\n","      <th>answer</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>What is the primary role of Robin Juhkental in...</td>\n","      <td>Robin Juhkental is the bassist of Malcolm Linc...</td>\n","      <td>Robin Juhkental is the keyboardist of Malcolm ...</td>\n","      <td>Robin Juhkental is the drummer of Malcolm Linc...</td>\n","      <td>Robin Juhkental is the lead singer of Malcolm ...</td>\n","      <td>Robin Juhkental is the lead guitarist of Malco...</td>\n","      <td>D</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Which of the following statements is true rega...</td>\n","      <td>The theory of relativity only encompasses one ...</td>\n","      <td>Special relativity explains the law of gravita...</td>\n","      <td>The theory of relativity does not encompass an...</td>\n","      <td>Special relativity applies to the cosmological...</td>\n","      <td>General relativity only applies to the motion ...</td>\n","      <td>D</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>In which country was the 1920 collection of co...</td>\n","      <td>United States</td>\n","      <td>Germany</td>\n","      <td>Australia</td>\n","      <td>France</td>\n","      <td>England</td>\n","      <td>E</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>What is one of the areas that Shimon Dovid Cow...</td>\n","      <td>Environmental conservation, opposing deforesta...</td>\n","      <td>Homosexuality, looser abortion laws and volunt...</td>\n","      <td>Freedom of speech, advocating for increased li...</td>\n","      <td>Gun control, advocating for stricter regulatio...</td>\n","      <td>Animal rights, opposing the use of animals for...</td>\n","      <td>B</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>When did the Dirt Road Diaries Tour begin and ...</td>\n","      <td>February 17, 2014 - November 26, 2014</td>\n","      <td>January 17, 2014 - October 26, 2014</td>\n","      <td>February 17, 2013 - November 26, 2013</td>\n","      <td>March 17, 2013 - December 26, 2013</td>\n","      <td>January 17, 2013 - October 26, 2013</td>\n","      <td>E</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                              prompt  \\\n","0  What is the primary role of Robin Juhkental in...   \n","1  Which of the following statements is true rega...   \n","2  In which country was the 1920 collection of co...   \n","3  What is one of the areas that Shimon Dovid Cow...   \n","4  When did the Dirt Road Diaries Tour begin and ...   \n","\n","                                                   A  \\\n","0  Robin Juhkental is the bassist of Malcolm Linc...   \n","1  The theory of relativity only encompasses one ...   \n","2                                      United States   \n","3  Environmental conservation, opposing deforesta...   \n","4              February 17, 2014 - November 26, 2014   \n","\n","                                                   B  \\\n","0  Robin Juhkental is the keyboardist of Malcolm ...   \n","1  Special relativity explains the law of gravita...   \n","2                                            Germany   \n","3  Homosexuality, looser abortion laws and volunt...   \n","4                January 17, 2014 - October 26, 2014   \n","\n","                                                   C  \\\n","0  Robin Juhkental is the drummer of Malcolm Linc...   \n","1  The theory of relativity does not encompass an...   \n","2                                          Australia   \n","3  Freedom of speech, advocating for increased li...   \n","4              February 17, 2013 - November 26, 2013   \n","\n","                                                   D  \\\n","0  Robin Juhkental is the lead singer of Malcolm ...   \n","1  Special relativity applies to the cosmological...   \n","2                                             France   \n","3  Gun control, advocating for stricter regulatio...   \n","4                 March 17, 2013 - December 26, 2013   \n","\n","                                                   E answer  \n","0  Robin Juhkental is the lead guitarist of Malco...      D  \n","1  General relativity only applies to the motion ...      D  \n","2                                            England      E  \n","3  Animal rights, opposing the use of animals for...      B  \n","4                January 17, 2013 - October 26, 2013      E  "]},"execution_count":157,"metadata":{},"output_type":"execute_result"}],"source":["df_train.head()"]},{"cell_type":"code","execution_count":158,"metadata":{},"outputs":[{"data":{"text/plain":["0        False\n","1        False\n","2        False\n","3        False\n","4        False\n","         ...  \n","26895    False\n","26896    False\n","26897    False\n","26898    False\n","26899    False\n","Length: 26900, dtype: bool"]},"execution_count":158,"metadata":{},"output_type":"execute_result"}],"source":["df_train.duplicated()"]},{"cell_type":"markdown","metadata":{},"source":["Now that we have gone from 200 -> 700 train examples, let us preprocess the data and begin training."]},{"cell_type":"code","execution_count":159,"metadata":{"execution":{"iopub.execute_input":"2023-07-21T12:18:12.439205Z","iopub.status.busy":"2023-07-21T12:18:12.438875Z","iopub.status.idle":"2023-07-21T12:18:12.452574Z","shell.execute_reply":"2023-07-21T12:18:12.45135Z","shell.execute_reply.started":"2023-07-21T12:18:12.439179Z"},"trusted":true},"outputs":[],"source":["option_to_index = {option: idx for idx, option in enumerate('ABCDE')}\n","index_to_option = {v: k for k,v in option_to_index.items()}\n","# å°†promptå’Œæ¯ä¸ªé—®é¢˜é…å¯¹ï¼Œç›¸å½“äºconcantenate,ç„¶åtokenizationè½¬æ¢æˆä¸€ä¸ªä¸€ä¸ªinput_id,ç„¶åä¹Ÿè½¬æ¢æˆtoken_type_id, è¿™ä¸ªæ˜¯æ ‡å®šä»ç¬¬å‡ ä¸ªå­—æ¯å¼€å§‹ä¸ä¸€æ ·ï¼Œç„¶åæ ‡å®š1ä¸ºï¼Œç„¶åä¸¤ä¸ªidéƒ½æ¯”åŸæ–‡è¦é•¿\n","def preprocess(example):\n","    first_sentence = [str(example['prompt'])] * 5\n","    second_sentences = [str(example[option]) for option in 'ABCDE']\n","    tokenized_example = tokenizer(first_sentence, second_sentences, truncation=False)\n","    tokenized_example['label'] = option_to_index[example['answer']]\n","    return tokenized_example\n","# è¿™ä¸ªç±»çš„ä¸»è¦ä½œç”¨æ˜¯å°†å¤šé¡¹é€‰æ‹©ä»»åŠ¡çš„è¾“å…¥æ•°æ®æ•´ç†æˆé€‚åˆæ¨¡å‹è¾“å…¥çš„æ ¼å¼ã€‚\n","'''\n","class MyClass:\n","    def __init__(self, var_a, var_b):\n","        self.var_a = var_a\n","        self.var_b = var_b\n","\n","è½¬åŒ–æˆ\n","@dataclass\n","class MyClass:\n","    var_a: str\n","    var_b: str\n","'''\n","@dataclass # ä½¿ç”¨ @dataclass è£…é¥°å™¨å¯ä»¥å¤§å¤§ç®€åŒ–ç±»çš„å®šä¹‰ï¼Œé¿å…äº†æ‰‹åŠ¨ç¼–å†™å†—é•¿çš„åˆå§‹åŒ–æ–¹æ³•å’Œå…¶ä»–ç‰¹æ®Šæ–¹æ³•ã€‚å»æ‰äº†__init__æ–¹æ³•\n","class DataCollatorForMultipleChoice:\n","    tokenizer: PreTrainedTokenizerBase\n","    # Pad to the longest sequence in the batch (or no padding if only a single sequence if provided).\n","    padding: Union[bool, str, PaddingStrategy] = True \n","    # Pad to a maximum length specified with the argument max_length or to the maximum\n","    # acceptable input length for the model if that argument is not provided.\n","    max_length: Optional[int] = None \n","    # If set will pad the sequence to a multiple of the provided value\n","    pad_to_multiple_of: Optional[int] = None\n","    # __call__æ˜¯  PreTrainedTokenizerBaseâ€™s encoding methods\n","    def __call__(self, features):\n","        # ç°åœ¨è¿˜æœ‰label\n","        label_name = 'label' if 'label' in features[0].keys() else 'labels'\n","        # # popå®Œç°åœ¨å·²ç»æ²¡æœ‰labeläº†\n","        labels = [feature.pop(label_name) for feature in features]\n","        batch_size = len(features)\n","        num_choices = len(features[0]['input_ids'])\n","        flattened_features = [\n","            [{k: v[i] for k, v in feature.items()} for i in range(num_choices)] for feature in features\n","        ]\n","        flattened_features = sum(flattened_features, [])# å¤šäº†ä¸€ä¸ªå¤–æ‹¬å·è¦å»æ‰ï¼Œå¹¶æ²¡æœ‰ç›¸åŠ ä»»ä½•æ•°\n","        # pad 0,ä¸”æŠŠç›¸åŒçš„keyçš„valueå€¼åˆå¹¶äº†\n","        batch = self.tokenizer.pad(\n","            flattened_features,\n","            padding=self.padding,\n","            max_length=self.max_length,\n","            pad_to_multiple_of=self.pad_to_multiple_of,\n","            return_tensors='pt',# å®ƒæŒ‡å®šäº†å¡«å……åçš„æ•°æ®åº”è¯¥ä»¥ PyTorch å¼ é‡çš„å½¢å¼è¿”å›ã€‚\n","        )\n","        #å˜tensorä¸ºdict\n","        batch = {k: v.view(batch_size, num_choices, -1) for k, v in batch.items()}\n","        # æ·»åŠ label\n","        batch['labels'] = torch.tensor(labels, dtype=torch.int64)\n","        #  dict,ä½†æ˜¯dé‡Œé¢ æ¯ä¸ªkeyå¯¹åº”çš„valueæ˜¯'torch.Tensor'\n","        return batch"]},{"cell_type":"markdown","metadata":{"execution":{"iopub.execute_input":"2023-07-21T03:38:13.805215Z","iopub.status.busy":"2023-07-21T03:38:13.804849Z","iopub.status.idle":"2023-07-21T03:38:13.814629Z","shell.execute_reply":"2023-07-21T03:38:13.813302Z","shell.execute_reply.started":"2023-07-21T03:38:13.805184Z"}},"source":["We first create a HuggingFace `Dataset`."]},{"cell_type":"code","execution_count":160,"metadata":{"execution":{"iopub.execute_input":"2023-07-21T12:18:12.455333Z","iopub.status.busy":"2023-07-21T12:18:12.453959Z","iopub.status.idle":"2023-07-21T12:18:13.778376Z","shell.execute_reply":"2023-07-21T12:18:13.777152Z","shell.execute_reply.started":"2023-07-21T12:18:12.455296Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","/home/krisfeng/anaconda3/envs/nlp/lib/python3.9/site-packages/transformers/convert_slow_tokenizer.py:470: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n","  warnings.warn(\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]},{"data":{"text/plain":["Dataset({\n","    features: ['prompt', 'A', 'B', 'C', 'D', 'E', 'answer'],\n","    num_rows: 26900\n","})"]},"execution_count":160,"metadata":{},"output_type":"execute_result"}],"source":["# ä½¿ç”¨Hugging Face Transformersåº“ä¸­çš„AutoTokenizeræ¥åŠ è½½ä¸€ä¸ªé¢„è®­ç»ƒçš„DeBERTa v3 Largeæ¨¡å‹çš„åˆ†è¯å™¨ï¼ˆtokenizerï¼‰\n","# åˆ†è¯å™¨ï¼ˆTokenizerï¼‰æ˜¯è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ä¸­çš„ä¸€ä¸ªé‡è¦å·¥å…·ï¼Œç”¨äºå°†æ–‡æœ¬æ•°æ®åˆ†å‰²æˆå•è¯ã€å­è¯æˆ–å…¶ä»–è¯­è¨€å•ä½çš„åºåˆ—ï¼Œ\n","# ä»¥ä¾¿è®¡ç®—æœºèƒ½å¤Ÿç†è§£å’Œå¤„ç†æ–‡æœ¬æ•°æ®ã€‚åœ¨NLPä»»åŠ¡ä¸­ï¼Œæ–‡æœ¬æ•°æ®é€šå¸¸ä»¥è¿ç»­çš„å­—ç¬¦åºåˆ—å½¢å¼å­˜åœ¨ï¼Œè€Œåˆ†è¯å™¨çš„ä½œç”¨æ˜¯å°†è¿™äº›å­—ç¬¦åºåˆ—åˆ‡åˆ†æˆç¦»æ•£çš„è¯­è¨€å•å…ƒï¼Œä»¥ä¾¿è¿›è¡Œè¯çº§åˆ«æˆ–å­è¯çº§åˆ«çš„å¤„ç†ã€‚\n","# attention maskä¸æ‡‚\n","\n","tokenizer = AutoTokenizer.from_pretrained(deberta_v3_large)\n","# è¿™è¡Œä»£ç çš„ä½œç”¨æ˜¯å°†åä¸º df_train çš„Pandas DataFrame è½¬åŒ–ä¸ºHugging Face Transformersåº“ä¸­çš„ Dataset å¯¹è±¡ã€‚\n","dataset = Dataset.from_pandas(df_train)\n","dataset"]},{"cell_type":"code","execution_count":161,"metadata":{},"outputs":[{"data":{"text/plain":["Dataset({\n","    features: ['prompt', 'A', 'B', 'C', 'D', 'E', 'answer'],\n","    num_rows: 200\n","})"]},"execution_count":161,"metadata":{},"output_type":"execute_result"}],"source":["dataset_valid = Dataset.from_pandas(df_valid)\n","dataset_valid"]},{"cell_type":"code","execution_count":162,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26900/26900 [00:07<00:00, 3383.55 examples/s]\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>prompt</th>\n","      <th>A</th>\n","      <th>B</th>\n","      <th>C</th>\n","      <th>D</th>\n","      <th>E</th>\n","      <th>answer</th>\n","      <th>input_ids</th>\n","      <th>token_type_ids</th>\n","      <th>attention_mask</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>What is the primary role of Robin Juhkental in...</td>\n","      <td>Robin Juhkental is the bassist of Malcolm Linc...</td>\n","      <td>Robin Juhkental is the keyboardist of Malcolm ...</td>\n","      <td>Robin Juhkental is the drummer of Malcolm Linc...</td>\n","      <td>Robin Juhkental is the lead singer of Malcolm ...</td>\n","      <td>Robin Juhkental is the lead guitarist of Malco...</td>\n","      <td>D</td>\n","      <td>[[1, 458, 269, 262, 1862, 985, 265, 8175, 1433...</td>\n","      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n","      <td>[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Which of the following statements is true rega...</td>\n","      <td>The theory of relativity only encompasses one ...</td>\n","      <td>Special relativity explains the law of gravita...</td>\n","      <td>The theory of relativity does not encompass an...</td>\n","      <td>Special relativity applies to the cosmological...</td>\n","      <td>General relativity only applies to the motion ...</td>\n","      <td>D</td>\n","      <td>[[1, 2597, 265, 262, 776, 3741, 269, 980, 1712...</td>\n","      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n","      <td>[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>In which country was the 1920 collection of co...</td>\n","      <td>United States</td>\n","      <td>Germany</td>\n","      <td>Australia</td>\n","      <td>France</td>\n","      <td>England</td>\n","      <td>E</td>\n","      <td>[[1, 344, 319, 658, 284, 262, 8547, 1283, 265,...</td>\n","      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n","      <td>[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>What is one of the areas that Shimon Dovid Cow...</td>\n","      <td>Environmental conservation, opposing deforesta...</td>\n","      <td>Homosexuality, looser abortion laws and volunt...</td>\n","      <td>Freedom of speech, advocating for increased li...</td>\n","      <td>Gun control, advocating for stricter regulatio...</td>\n","      <td>Animal rights, opposing the use of animals for...</td>\n","      <td>B</td>\n","      <td>[[1, 458, 269, 311, 265, 262, 893, 272, 81398,...</td>\n","      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n","      <td>[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>When did the Dirt Road Diaries Tour begin and ...</td>\n","      <td>February 17, 2014 - November 26, 2014</td>\n","      <td>January 17, 2014 - October 26, 2014</td>\n","      <td>February 17, 2013 - November 26, 2013</td>\n","      <td>March 17, 2013 - December 26, 2013</td>\n","      <td>January 17, 2013 - October 26, 2013</td>\n","      <td>E</td>\n","      <td>[[1, 486, 464, 262, 27469, 2055, 40512, 4590, ...</td>\n","      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,...</td>\n","      <td>[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...</td>\n","      <td>4</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                              prompt  \\\n","0  What is the primary role of Robin Juhkental in...   \n","1  Which of the following statements is true rega...   \n","2  In which country was the 1920 collection of co...   \n","3  What is one of the areas that Shimon Dovid Cow...   \n","4  When did the Dirt Road Diaries Tour begin and ...   \n","\n","                                                   A  \\\n","0  Robin Juhkental is the bassist of Malcolm Linc...   \n","1  The theory of relativity only encompasses one ...   \n","2                                      United States   \n","3  Environmental conservation, opposing deforesta...   \n","4              February 17, 2014 - November 26, 2014   \n","\n","                                                   B  \\\n","0  Robin Juhkental is the keyboardist of Malcolm ...   \n","1  Special relativity explains the law of gravita...   \n","2                                            Germany   \n","3  Homosexuality, looser abortion laws and volunt...   \n","4                January 17, 2014 - October 26, 2014   \n","\n","                                                   C  \\\n","0  Robin Juhkental is the drummer of Malcolm Linc...   \n","1  The theory of relativity does not encompass an...   \n","2                                          Australia   \n","3  Freedom of speech, advocating for increased li...   \n","4              February 17, 2013 - November 26, 2013   \n","\n","                                                   D  \\\n","0  Robin Juhkental is the lead singer of Malcolm ...   \n","1  Special relativity applies to the cosmological...   \n","2                                             France   \n","3  Gun control, advocating for stricter regulatio...   \n","4                 March 17, 2013 - December 26, 2013   \n","\n","                                                   E answer  \\\n","0  Robin Juhkental is the lead guitarist of Malco...      D   \n","1  General relativity only applies to the motion ...      D   \n","2                                            England      E   \n","3  Animal rights, opposing the use of animals for...      B   \n","4                January 17, 2013 - October 26, 2013      E   \n","\n","                                           input_ids  \\\n","0  [[1, 458, 269, 262, 1862, 985, 265, 8175, 1433...   \n","1  [[1, 2597, 265, 262, 776, 3741, 269, 980, 1712...   \n","2  [[1, 344, 319, 658, 284, 262, 8547, 1283, 265,...   \n","3  [[1, 458, 269, 311, 265, 262, 893, 272, 81398,...   \n","4  [[1, 486, 464, 262, 27469, 2055, 40512, 4590, ...   \n","\n","                                      token_type_ids  \\\n","0  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...   \n","1  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...   \n","2  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...   \n","3  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...   \n","4  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,...   \n","\n","                                      attention_mask  label  \n","0  [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...      3  \n","1  [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...      3  \n","2  [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...      4  \n","3  [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...      1  \n","4  [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...      4  "]},"execution_count":162,"metadata":{},"output_type":"execute_result"}],"source":["# ç›´æ¥mapä¼šå¢åŠ columnæ•°é‡\n","tokenized_dataset1 = dataset.map(preprocess)\n","tokenized_dataset_pandas1 = tokenized_dataset1.to_pandas()\n","\n","tokenized_dataset_pandas1.head()"]},{"cell_type":"markdown","metadata":{},"source":["And let us now preprocess the examples for training."]},{"cell_type":"code","execution_count":163,"metadata":{},"outputs":[{"data":{"text/plain":["dict_keys(['prompt', 'A', 'B', 'C', 'D', 'E', 'answer', 'input_ids', 'token_type_ids', 'attention_mask', 'label'])"]},"execution_count":163,"metadata":{},"output_type":"execute_result"}],"source":["tokenized_dataset1[0].keys()"]},{"cell_type":"code","execution_count":164,"metadata":{},"outputs":[{"data":{"text/plain":["dict"]},"execution_count":164,"metadata":{},"output_type":"execute_result"}],"source":["type(tokenized_dataset1[0])"]},{"cell_type":"code","execution_count":165,"metadata":{"execution":{"iopub.execute_input":"2023-07-21T12:18:13.780864Z","iopub.status.busy":"2023-07-21T12:18:13.78041Z","iopub.status.idle":"2023-07-21T12:18:14.684855Z","shell.execute_reply":"2023-07-21T12:18:14.683889Z","shell.execute_reply.started":"2023-07-21T12:18:13.780824Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26900/26900 [00:07<00:00, 3467.38 examples/s]\n"]},{"data":{"text/plain":["Dataset({\n","    features: ['input_ids', 'token_type_ids', 'attention_mask', 'label'],\n","    num_rows: 26900\n","})"]},"execution_count":165,"metadata":{},"output_type":"execute_result"}],"source":["# input_ids: Construct a DeBERTa tokenizer. Based on byte-level Byte-Pair-Encoding.\n","# token type_id:Create a mask from the two sequences passed to be used in a sequence-pair classification task. A DeBERTa\n","tokenized_dataset = dataset.map(preprocess, remove_columns=['prompt', 'A', 'B', 'C', 'D', 'E', 'answer'])\n","tokenized_dataset_pandas = tokenized_dataset.to_pandas()\n","tokenized_dataset"]},{"cell_type":"code","execution_count":166,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["What is the primary role of Robin Juhkental in the band Malcolm Lincoln?\n","Robin Juhkental is the bassist of Malcolm Lincoln and is responsible for laying down the foundation of their music.\n","Robin Juhkental is the keyboardist of Malcolm Lincoln and adds atmospheric sounds to their music.\n","Robin Juhkental is the drummer of Malcolm Lincoln and keeps the beat for the band's songs.\n","Robin Juhkental is the lead singer of Malcolm Lincoln and provides vocals for the band's songs.\n","Robin Juhkental is the lead guitarist of Malcolm Lincoln and is responsible for creating unique guitar melodies and solos.\n"]}],"source":["print(df_train.prompt[0])\n","print(df_train.A[0])\n","print(df_train.B[0])\n","print(df_train.C[0])\n","print(df_train.D[0])\n","print(df_train.E[0])"]},{"cell_type":"code","execution_count":167,"metadata":{},"outputs":[{"data":{"text/plain":["array([array([    1,   458,   269,   262,  1862,   985,   265,  8175, 14334,\n","              41672, 91707,   267,   262,  1704, 15084,  6175,   302,     2,\n","               8175, 14334, 41672, 91707,   269,   262, 26623,   265, 15084,\n","               6175,   263,   269,  1744,   270,  9022,   444,   262,  3332,\n","                265,   308,   755,   260,     2], dtype=int32)              ,\n","       array([    1,   458,   269,   262,  1862,   985,   265,  8175, 14334,\n","              41672, 91707,   267,   262,  1704, 15084,  6175,   302,     2,\n","               8175, 14334, 41672, 91707,   269,   262, 67509,   265, 15084,\n","               6175,   263,  3814, 13123,  2163,   264,   308,   755,   260,\n","                  2], dtype=int32)                                          ,\n","       array([    1,   458,   269,   262,  1862,   985,   265,  8175, 14334,\n","              41672, 91707,   267,   262,  1704, 15084,  6175,   302,     2,\n","               8175, 14334, 41672, 91707,   269,   262, 16090,   265, 15084,\n","               6175,   263,  3425,   262,  2584,   270,   262,  1704,   280,\n","                268,  2397,   260,     2], dtype=int32)                     ,\n","       array([    1,   458,   269,   262,  1862,   985,   265,  8175, 14334,\n","              41672, 91707,   267,   262,  1704, 15084,  6175,   302,     2,\n","               8175, 14334, 41672, 91707,   269,   262,   917,  4804,   265,\n","              15084,  6175,   263,   888, 10023,   270,   262,  1704,   280,\n","                268,  2397,   260,     2], dtype=int32)                     ,\n","       array([    1,   458,   269,   262,  1862,   985,   265,  8175, 14334,\n","              41672, 91707,   267,   262,  1704, 15084,  6175,   302,     2,\n","               8175, 14334, 41672, 91707,   269,   262,   917, 14330,   265,\n","              15084,  6175,   263,   269,  1744,   270,  1512,   988,  4066,\n","              21052,   263, 39558,   260,     2], dtype=int32)              ],\n","      dtype=object)"]},"execution_count":167,"metadata":{},"output_type":"execute_result"}],"source":["# input_ids â€” List of token ids to be fed to a model.They are token indices, numerical representations of tokens building the sequences that will be used as input by the model.\n","tokenized_dataset_pandas.input_ids[0]"]},{"cell_type":"code","execution_count":170,"metadata":{},"outputs":[{"data":{"text/plain":["array([array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,\n","              1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n","             dtype=int8)                                                       ,\n","       array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,\n","              1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int8)        ,\n","       array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,\n","              1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int8),\n","       array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,\n","              1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int8),\n","       array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,\n","              1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n","             dtype=int8)                                                       ],\n","      dtype=object)"]},"execution_count":170,"metadata":{},"output_type":"execute_result"}],"source":["# List of token type ids to be fed to a model \n","'''\n","token\n","A part of a sentence, usually a word, but can also be a subword (non-common\n"," words are often split in subwords) or a punctuation symbol.\n","token Type IDs\n","Some modelsâ€™ purpose is to do classification on pairs of sentences or question answering.\n","We can use our tokenizer to automatically generate such a sentence by passing the two\n"," sequences to tokenizer as two arguments (and not a list, like before) like this\n"," \n","from transformers import BertTokenizer\n","\n","tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n","sequence_a = \"HuggingFace is based in NYC\"\n","sequence_b = \"Where is HuggingFace based?\"\n","\n","encoded_dict = tokenizer(sequence_a, sequence_b)\n","decoded = tokenizer.decode(encoded_dict[\"input_ids\"])\n","\n","which will return:\n","\n","print(decoded)\n","[CLS] HuggingFace is based in NYC [SEP] Where is HuggingFace based? [SEP]\n","\n","The first sequence, the â€œcontextâ€ used for the question, \n","has all its tokens represented by a 0, whereas the second sequence, \n","corresponding to the â€œquestionâ€, has all its tokens represented by a 1.\n","'''\n","tokenized_dataset_pandas.token_type_ids[0]"]},{"cell_type":"code","execution_count":171,"metadata":{},"outputs":[{"data":{"text/plain":["array([array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","              1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n","             dtype=int8)                                                       ,\n","       array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","              1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int8)        ,\n","       array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","              1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int8),\n","       array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","              1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int8),\n","       array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","              1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n","             dtype=int8)                                                       ],\n","      dtype=object)"]},"execution_count":171,"metadata":{},"output_type":"execute_result"}],"source":["# The attention mask is a binary tensor indicating the position of t\n","# The padded indices so that the model does not attend to them. \n","# For the BertTokenizer , 1 indicates a value that should be attended to, while 0 indicates a padded value.\n","'''\n","tokenizer.padding_side = \"left\"\n","tokenizer.pad_token = tokenizer.eos_token\n","\n","sentences = [\"It will rain in the\",\n","            \"I want to eat a big bowl of\",\n","            \"My dog is\"]\n","inputs = tokenizer(sentences, return_tensors=\"pt\", padding=True)\n","\n","output_sequences = gpt2.generate(**inputs)\n","\n","for seq in output_sequences:\n","    print(tokenizer.decode(seq))\n","    \n","{'input_ids': tensor([\n","    [50256, 50256, 50256,  1026,   481,  6290,   287,   262],\n","    [   40,   765,   284,  4483,   257,  1263,  9396,   286],\n","    [50256, 50256, 50256, 50256, 50256,  3666,  3290,   318]\n","  ]),\n","'attention_mask': tensor([\n","    [0, 0, 0, 1, 1, 1, 1, 1],\n","    [1, 1, 1, 1, 1, 1, 1, 1],\n","    [0, 0, 0, 0, 0, 1, 1, 1]\n","  ])}\n","'''\n","tokenized_dataset_pandas.attention_mask[0]"]},{"cell_type":"code","execution_count":207,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[3]\n","1\n","5\n","[[{'input_ids': [1, 458, 269, 262, 1862, 985, 265, 8175, 14334, 41672, 91707, 267, 262, 1704, 15084, 6175, 302, 2, 8175, 14334, 41672, 91707, 269, 262, 26623, 265, 15084, 6175, 263, 269, 1744, 270, 9022, 444, 262, 3332, 265, 308, 755, 260, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, {'input_ids': [1, 458, 269, 262, 1862, 985, 265, 8175, 14334, 41672, 91707, 267, 262, 1704, 15084, 6175, 302, 2, 8175, 14334, 41672, 91707, 269, 262, 67509, 265, 15084, 6175, 263, 3814, 13123, 2163, 264, 308, 755, 260, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, {'input_ids': [1, 458, 269, 262, 1862, 985, 265, 8175, 14334, 41672, 91707, 267, 262, 1704, 15084, 6175, 302, 2, 8175, 14334, 41672, 91707, 269, 262, 16090, 265, 15084, 6175, 263, 3425, 262, 2584, 270, 262, 1704, 280, 268, 2397, 260, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, {'input_ids': [1, 458, 269, 262, 1862, 985, 265, 8175, 14334, 41672, 91707, 267, 262, 1704, 15084, 6175, 302, 2, 8175, 14334, 41672, 91707, 269, 262, 917, 4804, 265, 15084, 6175, 263, 888, 10023, 270, 262, 1704, 280, 268, 2397, 260, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, {'input_ids': [1, 458, 269, 262, 1862, 985, 265, 8175, 14334, 41672, 91707, 267, 262, 1704, 15084, 6175, 302, 2, 8175, 14334, 41672, 91707, 269, 262, 917, 14330, 265, 15084, 6175, 263, 269, 1744, 270, 1512, 988, 4066, 21052, 263, 39558, 260, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}]]\n","[{'input_ids': [1, 458, 269, 262, 1862, 985, 265, 8175, 14334, 41672, 91707, 267, 262, 1704, 15084, 6175, 302, 2, 8175, 14334, 41672, 91707, 269, 262, 26623, 265, 15084, 6175, 263, 269, 1744, 270, 9022, 444, 262, 3332, 265, 308, 755, 260, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, {'input_ids': [1, 458, 269, 262, 1862, 985, 265, 8175, 14334, 41672, 91707, 267, 262, 1704, 15084, 6175, 302, 2, 8175, 14334, 41672, 91707, 269, 262, 67509, 265, 15084, 6175, 263, 3814, 13123, 2163, 264, 308, 755, 260, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, {'input_ids': [1, 458, 269, 262, 1862, 985, 265, 8175, 14334, 41672, 91707, 267, 262, 1704, 15084, 6175, 302, 2, 8175, 14334, 41672, 91707, 269, 262, 16090, 265, 15084, 6175, 263, 3425, 262, 2584, 270, 262, 1704, 280, 268, 2397, 260, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, {'input_ids': [1, 458, 269, 262, 1862, 985, 265, 8175, 14334, 41672, 91707, 267, 262, 1704, 15084, 6175, 302, 2, 8175, 14334, 41672, 91707, 269, 262, 917, 4804, 265, 15084, 6175, 263, 888, 10023, 270, 262, 1704, 280, 268, 2397, 260, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, {'input_ids': [1, 458, 269, 262, 1862, 985, 265, 8175, 14334, 41672, 91707, 267, 262, 1704, 15084, 6175, 302, 2, 8175, 14334, 41672, 91707, 269, 262, 917, 14330, 265, 15084, 6175, 263, 269, 1744, 270, 1512, 988, 4066, 21052, 263, 39558, 260, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}]\n","tensor([[    1,   458,   269,   262,  1862,   985,   265,  8175, 14334, 41672,\n","         91707,   267,   262,  1704, 15084,  6175,   302,     2,  8175, 14334,\n","         41672, 91707,   269,   262, 26623,   265, 15084,  6175,   263,   269,\n","          1744,   270,  9022,   444,   262,  3332,   265,   308,   755,   260,\n","             2],\n","        [    1,   458,   269,   262,  1862,   985,   265,  8175, 14334, 41672,\n","         91707,   267,   262,  1704, 15084,  6175,   302,     2,  8175, 14334,\n","         41672, 91707,   269,   262, 67509,   265, 15084,  6175,   263,  3814,\n","         13123,  2163,   264,   308,   755,   260,     2,     0,     0,     0,\n","             0],\n","        [    1,   458,   269,   262,  1862,   985,   265,  8175, 14334, 41672,\n","         91707,   267,   262,  1704, 15084,  6175,   302,     2,  8175, 14334,\n","         41672, 91707,   269,   262, 16090,   265, 15084,  6175,   263,  3425,\n","           262,  2584,   270,   262,  1704,   280,   268,  2397,   260,     2,\n","             0],\n","        [    1,   458,   269,   262,  1862,   985,   265,  8175, 14334, 41672,\n","         91707,   267,   262,  1704, 15084,  6175,   302,     2,  8175, 14334,\n","         41672, 91707,   269,   262,   917,  4804,   265, 15084,  6175,   263,\n","           888, 10023,   270,   262,  1704,   280,   268,  2397,   260,     2,\n","             0],\n","        [    1,   458,   269,   262,  1862,   985,   265,  8175, 14334, 41672,\n","         91707,   267,   262,  1704, 15084,  6175,   302,     2,  8175, 14334,\n","         41672, 91707,   269,   262,   917, 14330,   265, 15084,  6175,   263,\n","           269,  1744,   270,  1512,   988,  4066, 21052,   263, 39558,   260,\n","             2]])\n"]}],"source":["features = [tokenized_dataset[0]]\n","label_name = 'label' if 'label' in features[0].keys() else 'labels'\n","labels = [feature.pop(label_name) for feature in features]\n","print(labels)\n","batch_size = len(features)\n","print(batch_size)\n","num_choices = len(features[0]['input_ids'])\n","print(num_choices)\n","flattened_features = [\n","        [{k: v[i] for k, v in feature.items()} for i in range(num_choices)] for feature in features\n","    ]\n","print(flattened_features)# å¤šäº†ä¸€ä¸ªå¤–æ‹¬å·è¦å»æ‰\n","flattened_features = sum(flattened_features, [])# å¤šäº†ä¸€ä¸ªå¤–æ‹¬å·è¦å»æ‰ï¼Œå¹¶æ²¡æœ‰ç›¸åŠ ä»»ä½•æ•°\n","print(flattened_features)\n","# pad 0,ä¸”æŠŠç›¸åŒçš„keyçš„valueå€¼åˆå¹¶äº†\n","batch = tokenizer.pad(\n","    flattened_features,\n","    padding=True,\n","    max_length=None,\n","    pad_to_multiple_of=None,\n","    return_tensors='pt',# å®ƒæŒ‡å®šäº†å¡«å……åçš„æ•°æ®åº”è¯¥ä»¥ PyTorch å¼ é‡çš„å½¢å¼è¿”å›ã€‚\n",")\n","print(batch.input_ids)\n","batch = {k: v.view(batch_size, num_choices, -1) for k, v in batch.items()}\n","#print(batch)\n","batch['labels'] = torch.tensor(labels, dtype=torch.int64)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","tokenized_dataset_valid = dataset_valid.map(preprocess, remove_columns=['prompt', 'A', 'B', 'C', 'D', 'E', 'answer'])"]},{"cell_type":"markdown","metadata":{},"source":["# Training"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import numpy as np\n","def map_at_3(predictions, labels):\n","    map_sum = 0\n","    pred = np.argsort(-1*np.array(predictions),axis=1)[:,:3]\n","    for x,y in zip(pred,labels):\n","        z = [1/i if y==j else 0 for i,j in zip([1,2,3],x)]\n","        map_sum += np.sum(z)\n","    return map_sum / len(predictions)\n","\n","# Define your custom evaluation function\n","def compute_metrics(p):\n","    predictions = p.predictions.tolist()\n","    labels = p.label_ids.tolist()\n","    return {\"map@3\": map_at_3(predictions, labels)}\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-21T12:18:14.693881Z","iopub.status.busy":"2023-07-21T12:18:14.693512Z","iopub.status.idle":"2023-07-21T12:20:57.538832Z","shell.execute_reply":"2023-07-21T12:20:57.537531Z","shell.execute_reply.started":"2023-07-21T12:18:14.693846Z"},"trusted":true},"outputs":[],"source":["training_args = TrainingArguments(\n","    # å…·ä½“æ¥è¯´ï¼Œå­¦ä¹ ç‡é¢„çƒ­æ˜¯æŒ‡åœ¨è®­ç»ƒçš„å¼€å§‹é˜¶æ®µï¼Œé€æ¸å¢åŠ å­¦ä¹ ç‡çš„è¿‡ç¨‹ã€‚é€šå¸¸ï¼Œåœ¨è®­ç»ƒåˆæœŸ\n","    # æ¨¡å‹çš„æƒé‡éšæœºåˆå§‹åŒ–ï¼Œå¯¹æ•°æ®çš„æ‹Ÿåˆä¸å¤Ÿå¥½ï¼Œæ­¤æ—¶è¾ƒå°çš„å­¦ä¹ ç‡å¯ä»¥å¸®åŠ©æ¨¡å‹æ›´å¿«åœ°æ”¶æ•›\n","    # åˆ°ä¸€ä¸ªç›¸å¯¹åˆé€‚çš„æƒé‡çŠ¶æ€ã€‚ç„¶åï¼Œéšç€è®­ç»ƒçš„è¿›è¡Œï¼Œå­¦ä¹ ç‡é€æ¸å¢åŠ ï¼Œä»¥å…è®¸æ¨¡å‹åœ¨å±€éƒ¨æå°å€¼ä¹‹é—´è·³è·ƒï¼Œä»¥å¯»æ‰¾æ›´å¥½çš„å…¨å±€æœ€å°å€¼ã€‚\n","    warmup_ratio=0.8,\n","    learning_rate=5e-6,\n","    load_best_model_at_end=True,\n","    evaluation_strategy='steps',\n","    logging_steps=500,\n","    metric_for_best_model='map@3',# ç”¨äºé€‰æ‹©æœ€ä½³æ¨¡å‹çš„è¯„ä¼°æŒ‡æ ‡ã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œä¼šç›‘è§†æ­¤æŒ‡æ ‡çš„å˜åŒ–ï¼Œå¹¶é€‰æ‹©åœ¨éªŒè¯é›†ä¸Šå…·æœ‰æœ€ä½³è¯¥æŒ‡æ ‡å€¼çš„æ¨¡å‹ã€‚\n","    per_device_train_batch_size=1, # è¿™æ˜¯æŒ‡æ¯ä¸ªGPUè®¾å¤‡ä¸Šçš„è®­ç»ƒbatch sizeå¤§å°ã€‚å¦‚æœä½ æœ‰å¤šä¸ªGPUï¼Œä½ å¯ä»¥é€‰æ‹©åŒæ—¶åœ¨æ¯ä¸ªGPUä¸Šå¤„ç†å¤šä¸ªæ ·æœ¬ï¼Œä»¥åŠ é€Ÿè®­ç»ƒã€‚è¿™ä¸ªå‚æ•°ä¼šå†³å®šæ¯ä¸ªGPUä¸Šçš„æ‰¹æ¬¡å¤§å°ã€‚\n","    eval_steps=500,\n","    per_device_eval_batch_size=1,\n","    num_train_epochs=3,\n","    #report_to='none',\n","    output_dir='.',\n","    save_total_limit = 1,\n",")\n","\n","model = AutoModelForMultipleChoice.from_pretrained(deberta_v3_large)\n","'''\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    tokenizer=tokenizer,\n","    data_collator=DataCollatorForMultipleChoice(tokenizer=tokenizer),\n","    train_dataset=tokenized_dataset,\n","    compute_metrics = compute_metrics,\n",")\n","'''\n","# hugging face API for training using pytorch\n","# Hugging Face Transformersåº“ä¸­çš„ Trainer å¯¹è±¡ï¼Œç”¨äºè®­ç»ƒå’Œè¯„ä¼°NLPæ¨¡å‹\n","trainer = Trainer(\n","    model=model,# can be transformer pretrain model\n","    args=training_args,# è¿™æ˜¯ä¸€ä¸ªåŒ…å«è®­ç»ƒå‚æ•°çš„é…ç½®å¯¹è±¡ï¼Œå…¶ä¸­åŒ…æ‹¬è¯¸å¦‚è®­ç»ƒæ­¥éª¤æ•°ã€å­¦ä¹ ç‡ã€æ‰¹å¤„ç†å¤§å°ç­‰è®­ç»ƒè¶…å‚æ•°çš„è®¾ç½®ã€‚\n","    tokenizer=tokenizer,# è¿™æ˜¯ç”¨äºåˆ†è¯æ–‡æœ¬çš„åˆ†è¯å™¨ï¼Œå®ƒä¼šå°†è¾“å…¥æ•°æ®åˆ†è¯æˆæ¨¡å‹å¯æ¥å—çš„å½¢å¼ã€‚\n","    data_collator=DataCollatorForMultipleChoice(tokenizer=tokenizer),# Batch encoding, ä¸“é—¨ç”¨äºå¤„ç†å¤šé€‰é¢˜ç±»å‹çš„æ•°æ®é›†,è¿™æ˜¯ä¸€ä¸ªæ•°æ®æ”¶é›†å™¨ï¼Œç”¨äºå°†åˆ†è¯åçš„æ•°æ®æ•´ç†æˆé€‚åˆæ¨¡å‹è¾“å…¥çš„å½¢å¼ã€‚å› ä¸ºæ­¤æ—¶tokenizeddå æ•°æ®éƒ½ä¸ç­‰é•¿ï¼Œå¾—paddingæ‰è¡Œ\n","    train_dataset=tokenized_dataset,# è¿™æ˜¯è®­ç»ƒæ•°æ®é›†ï¼Œé€šå¸¸åŒ…å«äº†ç»è¿‡åˆ†è¯å’Œé¢„å¤„ç†åçš„æ•°æ®ã€‚\n","    eval_dataset=tokenized_dataset_valid, # è¿™æ˜¯ç”¨äºæ¨¡å‹éªŒè¯çš„æ•°æ®é›†ï¼Œé€šå¸¸ä¹ŸåŒ…å«äº†ç»è¿‡åˆ†è¯å’Œé¢„å¤„ç†åçš„æ•°æ®ã€‚\n","    compute_metrics = compute_metrics,# è¿™æ˜¯ä¸€ä¸ªå‡½æ•°ï¼Œç”¨äºè®¡ç®—è¯„ä¼°æŒ‡æ ‡ã€‚ä½ å¯ä»¥è‡ªå®šä¹‰æ­¤å‡½æ•°ï¼Œä»¥æ ¹æ®ä½ çš„ä»»åŠ¡è®¡ç®—é€‚å½“çš„æŒ‡æ ‡ï¼Œä¾‹å¦‚å‡†ç¡®åº¦ã€F1åˆ†æ•°ç­‰ã€‚\n",")\n","\n","trainer.train()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["# Predicting on the test set"]},{"cell_type":"markdown","metadata":{},"source":["Now that we have trained our model, let us predict on the test set."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["test_df = pd.read_csv('./kaggle/input/train.csv')\n","test_df['answer'] = 'A' # dummy answer that allows us to preprocess the test dataset just like we preprocessed the train dataset\n","\n","tokenized_test_dataset = Dataset.from_pandas(test_df.drop(columns=['id'])).map(preprocess, remove_columns=['prompt', 'A', 'B', 'C', 'D', 'E'])"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["test_predictions = trainer.predict(tokenized_test_dataset).predictions\n","test_predictions[:4]"]},{"cell_type":"markdown","metadata":{},"source":["The predictions are values output from the last layer of our neural network.\n","\n","Let's obtain the predicted answer ids by sorting them from largest to the smallest."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["predictions_as_ids = np.argsort(-test_predictions, 1)\n","predictions_as_ids[:3]"]},{"cell_type":"markdown","metadata":{},"source":["Let us now assign a letter corresponding to each predicted id (0 -> 'A', 1 -> 'B', etc). "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["predictions_as_answer_letters = np.array(list('ABCDE'))[predictions_as_ids]\n","predictions_as_answer_letters[:3]"]},{"cell_type":"markdown","metadata":{},"source":["And let us now go from this representation to outputting a string with 3 highest rated answers seperated by a space."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["predictions_as_string = test_df['prediction'] = [\n","    ' '.join(row) for row in predictions_as_answer_letters[:, :3]\n","]\n","predictions_as_string[:3]"]},{"cell_type":"markdown","metadata":{},"source":["And we are done! ğŸ¥³\n","\n","Let us now output our submission."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["submission = test_df[['id', 'prediction']]\n","submission.to_csv('submission.csv', index=False)\n","\n","pd.read_csv('submission.csv').head()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["I hope you enjoyed this notebook!\n","\n","If you found it useful, please upvote ğŸ‘‰ [the corresponding dataset with 500 new training examples](https://www.kaggle.com/datasets/radek1/additional-train-data-for-llm-science-exam) ğŸ‘ˆ\n","\n","Thank you, appreciate your help! ğŸ™ğŸ˜Š\n","\n","Thank you for reading and happy Kaggling!"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.17"}},"nbformat":4,"nbformat_minor":4}
