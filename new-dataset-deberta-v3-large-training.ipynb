{"cells":[{"cell_type":"markdown","metadata":{},"source":["\n","200 examples in the public dataset leaves very little room for training!\n","\n","Using `gpt-3.5-turbo` I created another 500 high quality examples at my expense [that I share freely here](https://www.kaggle.com/datasets/radek1/additional-train-data-for-llm-science-exam). They are what (as of now) pushes this notebook to the highest achieved score across the public notebooks (`0.723`)!\n","\n","If you find the additional training examples useful, please upvote the dataset! 😊\n","\n","👉 [additional train data for LLM Science Exam 🥳](https://www.kaggle.com/datasets/radek1/additional-train-data-for-llm-science-exam)\n","\n","Thank you! Appreciate your help! 🙏🙏🙏\n","\n","This notebook builds on a [notebook](https://www.kaggle.com/code/leonidkulyk/lb-0-709-llm-se-deberta-v3-large-i-1k-wiki) by LEONID KULYK. Among some of the changes are:\n","\n","* use of a new high quality dataset for training\n","* modified training procedure carried out in the notebook\n","* general streamlining of code/training for readability\n","\n","**A couple of related resources you might find useful:**\n","\n","* [📊 15k high-quality train examples 🏆🔥🚀](https://www.kaggle.com/datasets/radek1/15k-high-quality-examples) - another 15 000 examples I created to help you grow that train/validation set of yours and improve results\n","* [📊 Best Open Source LLM Starter Pack 🧙🚀](https://www.kaggle.com/datasets/radek1/best-llm-starter-pack) -- the largest (and best) open source model I managed to run on Kaggle!\n","* [Science Exam Trained Model Weights 🚀](https://www.kaggle.com/datasets/radek1/science-exam-trained-model-weights)"]},{"cell_type":"markdown","metadata":{},"source":["# Preprocessing the dataset"]},{"cell_type":"code","execution_count":152,"metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2023-07-21T12:18:05.52768Z","iopub.status.busy":"2023-07-21T12:18:05.527036Z","iopub.status.idle":"2023-07-21T12:18:12.391158Z","shell.execute_reply":"2023-07-21T12:18:12.389855Z","shell.execute_reply.started":"2023-07-21T12:18:05.527645Z"},"trusted":true},"outputs":[],"source":["from typing import Optional, Union\n","import pandas as pd\n","import numpy as np\n","import torch\n","from datasets import Dataset\n","from dataclasses import dataclass # 于简化创建和管理包含数据的类。dataclass 装饰器是 dataclasses 模块的一部分，它允许你定义一个类，其中的属性（成员变量）可以轻松地自动生成初始化方法、\n","from transformers import AutoTokenizer\n","from transformers.tokenization_utils_base import PreTrainedTokenizerBase, PaddingStrategy\n","from transformers import AutoModelForMultipleChoice, TrainingArguments, Trainer, AutoModel\n","\n","deberta_v3_large = '/home/krisfeng/code/llm/kaggle/input/deberta-v3-large-hf-weights'"]},{"cell_type":"code","execution_count":153,"metadata":{},"outputs":[],"source":["torch.cuda.empty_cache()"]},{"cell_type":"markdown","metadata":{},"source":["We begin by loading and processing the train data."]},{"cell_type":"code","execution_count":154,"metadata":{"execution":{"iopub.execute_input":"2023-07-21T12:18:12.393051Z","iopub.status.busy":"2023-07-21T12:18:12.392674Z","iopub.status.idle":"2023-07-21T12:18:12.414514Z","shell.execute_reply":"2023-07-21T12:18:12.413788Z","shell.execute_reply.started":"2023-07-21T12:18:12.393005Z"},"trusted":true},"outputs":[{"data":{"text/plain":["(500, 7)"]},"execution_count":154,"metadata":{},"output_type":"execute_result"}],"source":["df_train = pd.read_csv('./kaggle/input/extra_train_set.csv')\n","#df_train = df_train.drop(columns=\"id\")\n","df_train.shape"]},{"cell_type":"code","execution_count":155,"metadata":{},"outputs":[],"source":["df_valid = pd.read_csv('kaggle/input/train.csv')\n","df_valid = df_valid.drop(columns=\"id\")"]},{"cell_type":"markdown","metadata":{},"source":["Let's add another 500 examples to the train set!"]},{"cell_type":"code","execution_count":156,"metadata":{"execution":{"iopub.execute_input":"2023-07-21T12:18:12.416841Z","iopub.status.busy":"2023-07-21T12:18:12.415834Z","iopub.status.idle":"2023-07-21T12:18:12.435184Z","shell.execute_reply":"2023-07-21T12:18:12.434346Z","shell.execute_reply.started":"2023-07-21T12:18:12.416804Z"},"trusted":true},"outputs":[{"data":{"text/plain":["(26900, 7)"]},"execution_count":156,"metadata":{},"output_type":"execute_result"}],"source":["df_train = pd.concat([\n","    pd.read_csv('./kaggle/input/6000_train_examples.csv'),\n","    pd.read_csv('./kaggle/input/15k_gpt3.5-turbo.csv'),\n","    pd.read_csv('./kaggle/input/5900_examples.csv'),\n","])\n","df_train.reset_index(inplace=True, drop=True)\n","df_train.shape"]},{"cell_type":"code","execution_count":157,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>prompt</th>\n","      <th>A</th>\n","      <th>B</th>\n","      <th>C</th>\n","      <th>D</th>\n","      <th>E</th>\n","      <th>answer</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>What is the primary role of Robin Juhkental in...</td>\n","      <td>Robin Juhkental is the bassist of Malcolm Linc...</td>\n","      <td>Robin Juhkental is the keyboardist of Malcolm ...</td>\n","      <td>Robin Juhkental is the drummer of Malcolm Linc...</td>\n","      <td>Robin Juhkental is the lead singer of Malcolm ...</td>\n","      <td>Robin Juhkental is the lead guitarist of Malco...</td>\n","      <td>D</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Which of the following statements is true rega...</td>\n","      <td>The theory of relativity only encompasses one ...</td>\n","      <td>Special relativity explains the law of gravita...</td>\n","      <td>The theory of relativity does not encompass an...</td>\n","      <td>Special relativity applies to the cosmological...</td>\n","      <td>General relativity only applies to the motion ...</td>\n","      <td>D</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>In which country was the 1920 collection of co...</td>\n","      <td>United States</td>\n","      <td>Germany</td>\n","      <td>Australia</td>\n","      <td>France</td>\n","      <td>England</td>\n","      <td>E</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>What is one of the areas that Shimon Dovid Cow...</td>\n","      <td>Environmental conservation, opposing deforesta...</td>\n","      <td>Homosexuality, looser abortion laws and volunt...</td>\n","      <td>Freedom of speech, advocating for increased li...</td>\n","      <td>Gun control, advocating for stricter regulatio...</td>\n","      <td>Animal rights, opposing the use of animals for...</td>\n","      <td>B</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>When did the Dirt Road Diaries Tour begin and ...</td>\n","      <td>February 17, 2014 - November 26, 2014</td>\n","      <td>January 17, 2014 - October 26, 2014</td>\n","      <td>February 17, 2013 - November 26, 2013</td>\n","      <td>March 17, 2013 - December 26, 2013</td>\n","      <td>January 17, 2013 - October 26, 2013</td>\n","      <td>E</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                              prompt  \\\n","0  What is the primary role of Robin Juhkental in...   \n","1  Which of the following statements is true rega...   \n","2  In which country was the 1920 collection of co...   \n","3  What is one of the areas that Shimon Dovid Cow...   \n","4  When did the Dirt Road Diaries Tour begin and ...   \n","\n","                                                   A  \\\n","0  Robin Juhkental is the bassist of Malcolm Linc...   \n","1  The theory of relativity only encompasses one ...   \n","2                                      United States   \n","3  Environmental conservation, opposing deforesta...   \n","4              February 17, 2014 - November 26, 2014   \n","\n","                                                   B  \\\n","0  Robin Juhkental is the keyboardist of Malcolm ...   \n","1  Special relativity explains the law of gravita...   \n","2                                            Germany   \n","3  Homosexuality, looser abortion laws and volunt...   \n","4                January 17, 2014 - October 26, 2014   \n","\n","                                                   C  \\\n","0  Robin Juhkental is the drummer of Malcolm Linc...   \n","1  The theory of relativity does not encompass an...   \n","2                                          Australia   \n","3  Freedom of speech, advocating for increased li...   \n","4              February 17, 2013 - November 26, 2013   \n","\n","                                                   D  \\\n","0  Robin Juhkental is the lead singer of Malcolm ...   \n","1  Special relativity applies to the cosmological...   \n","2                                             France   \n","3  Gun control, advocating for stricter regulatio...   \n","4                 March 17, 2013 - December 26, 2013   \n","\n","                                                   E answer  \n","0  Robin Juhkental is the lead guitarist of Malco...      D  \n","1  General relativity only applies to the motion ...      D  \n","2                                            England      E  \n","3  Animal rights, opposing the use of animals for...      B  \n","4                January 17, 2013 - October 26, 2013      E  "]},"execution_count":157,"metadata":{},"output_type":"execute_result"}],"source":["df_train.head()"]},{"cell_type":"code","execution_count":158,"metadata":{},"outputs":[{"data":{"text/plain":["0        False\n","1        False\n","2        False\n","3        False\n","4        False\n","         ...  \n","26895    False\n","26896    False\n","26897    False\n","26898    False\n","26899    False\n","Length: 26900, dtype: bool"]},"execution_count":158,"metadata":{},"output_type":"execute_result"}],"source":["df_train.duplicated()"]},{"cell_type":"markdown","metadata":{},"source":["Now that we have gone from 200 -> 700 train examples, let us preprocess the data and begin training."]},{"cell_type":"code","execution_count":159,"metadata":{"execution":{"iopub.execute_input":"2023-07-21T12:18:12.439205Z","iopub.status.busy":"2023-07-21T12:18:12.438875Z","iopub.status.idle":"2023-07-21T12:18:12.452574Z","shell.execute_reply":"2023-07-21T12:18:12.45135Z","shell.execute_reply.started":"2023-07-21T12:18:12.439179Z"},"trusted":true},"outputs":[],"source":["option_to_index = {option: idx for idx, option in enumerate('ABCDE')}\n","index_to_option = {v: k for k,v in option_to_index.items()}\n","# 将prompt和每个问题配对，相当于concantenate,然后tokenization转换成一个一个input_id,然后也转换成token_type_id, 这个是标定从第几个字母开始不一样，然后标定1为，然后两个id都比原文要长\n","def preprocess(example):\n","    first_sentence = [str(example['prompt'])] * 5\n","    second_sentences = [str(example[option]) for option in 'ABCDE']\n","    tokenized_example = tokenizer(first_sentence, second_sentences, truncation=False)\n","    tokenized_example['label'] = option_to_index[example['answer']]\n","    return tokenized_example\n","# 这个类的主要作用是将多项选择任务的输入数据整理成适合模型输入的格式。\n","'''\n","class MyClass:\n","    def __init__(self, var_a, var_b):\n","        self.var_a = var_a\n","        self.var_b = var_b\n","\n","转化成\n","@dataclass\n","class MyClass:\n","    var_a: str\n","    var_b: str\n","'''\n","@dataclass # 使用 @dataclass 装饰器可以大大简化类的定义，避免了手动编写冗长的初始化方法和其他特殊方法。去掉了__init__方法\n","class DataCollatorForMultipleChoice:\n","    tokenizer: PreTrainedTokenizerBase\n","    # Pad to the longest sequence in the batch (or no padding if only a single sequence if provided).\n","    padding: Union[bool, str, PaddingStrategy] = True \n","    # Pad to a maximum length specified with the argument max_length or to the maximum\n","    # acceptable input length for the model if that argument is not provided.\n","    max_length: Optional[int] = None \n","    # If set will pad the sequence to a multiple of the provided value\n","    pad_to_multiple_of: Optional[int] = None\n","    # __call__是  PreTrainedTokenizerBase’s encoding methods\n","    def __call__(self, features):\n","        # 现在还有label\n","        label_name = 'label' if 'label' in features[0].keys() else 'labels'\n","        # # pop完现在已经没有label了\n","        labels = [feature.pop(label_name) for feature in features]\n","        batch_size = len(features)\n","        num_choices = len(features[0]['input_ids'])\n","        flattened_features = [\n","            [{k: v[i] for k, v in feature.items()} for i in range(num_choices)] for feature in features\n","        ]\n","        flattened_features = sum(flattened_features, [])# 多了一个外括号要去掉，并没有相加任何数\n","        # pad 0,且把相同的key的value值合并了\n","        batch = self.tokenizer.pad(\n","            flattened_features,\n","            padding=self.padding,\n","            max_length=self.max_length,\n","            pad_to_multiple_of=self.pad_to_multiple_of,\n","            return_tensors='pt',# 它指定了填充后的数据应该以 PyTorch 张量的形式返回。\n","        )\n","        #变tensor为dict\n","        batch = {k: v.view(batch_size, num_choices, -1) for k, v in batch.items()}\n","        # 添加label\n","        batch['labels'] = torch.tensor(labels, dtype=torch.int64)\n","        #  dict,但是d里面 每个key对应的value是'torch.Tensor'\n","        return batch"]},{"cell_type":"markdown","metadata":{"execution":{"iopub.execute_input":"2023-07-21T03:38:13.805215Z","iopub.status.busy":"2023-07-21T03:38:13.804849Z","iopub.status.idle":"2023-07-21T03:38:13.814629Z","shell.execute_reply":"2023-07-21T03:38:13.813302Z","shell.execute_reply.started":"2023-07-21T03:38:13.805184Z"}},"source":["We first create a HuggingFace `Dataset`."]},{"cell_type":"code","execution_count":160,"metadata":{"execution":{"iopub.execute_input":"2023-07-21T12:18:12.455333Z","iopub.status.busy":"2023-07-21T12:18:12.453959Z","iopub.status.idle":"2023-07-21T12:18:13.778376Z","shell.execute_reply":"2023-07-21T12:18:13.777152Z","shell.execute_reply.started":"2023-07-21T12:18:12.455296Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","/home/krisfeng/anaconda3/envs/nlp/lib/python3.9/site-packages/transformers/convert_slow_tokenizer.py:470: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n","  warnings.warn(\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]},{"data":{"text/plain":["Dataset({\n","    features: ['prompt', 'A', 'B', 'C', 'D', 'E', 'answer'],\n","    num_rows: 26900\n","})"]},"execution_count":160,"metadata":{},"output_type":"execute_result"}],"source":["# 使用Hugging Face Transformers库中的AutoTokenizer来加载一个预训练的DeBERTa v3 Large模型的分词器（tokenizer）\n","# 分词器（Tokenizer）是自然语言处理（NLP）中的一个重要工具，用于将文本数据分割成单词、子词或其他语言单位的序列，\n","# 以便计算机能够理解和处理文本数据。在NLP任务中，文本数据通常以连续的字符序列形式存在，而分词器的作用是将这些字符序列切分成离散的语言单元，以便进行词级别或子词级别的处理。\n","# attention mask不懂\n","\n","tokenizer = AutoTokenizer.from_pretrained(deberta_v3_large)\n","# 这行代码的作用是将名为 df_train 的Pandas DataFrame 转化为Hugging Face Transformers库中的 Dataset 对象。\n","dataset = Dataset.from_pandas(df_train)\n","dataset"]},{"cell_type":"code","execution_count":161,"metadata":{},"outputs":[{"data":{"text/plain":["Dataset({\n","    features: ['prompt', 'A', 'B', 'C', 'D', 'E', 'answer'],\n","    num_rows: 200\n","})"]},"execution_count":161,"metadata":{},"output_type":"execute_result"}],"source":["dataset_valid = Dataset.from_pandas(df_valid)\n","dataset_valid"]},{"cell_type":"code","execution_count":162,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Map: 100%|██████████| 26900/26900 [00:07<00:00, 3383.55 examples/s]\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>prompt</th>\n","      <th>A</th>\n","      <th>B</th>\n","      <th>C</th>\n","      <th>D</th>\n","      <th>E</th>\n","      <th>answer</th>\n","      <th>input_ids</th>\n","      <th>token_type_ids</th>\n","      <th>attention_mask</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>What is the primary role of Robin Juhkental in...</td>\n","      <td>Robin Juhkental is the bassist of Malcolm Linc...</td>\n","      <td>Robin Juhkental is the keyboardist of Malcolm ...</td>\n","      <td>Robin Juhkental is the drummer of Malcolm Linc...</td>\n","      <td>Robin Juhkental is the lead singer of Malcolm ...</td>\n","      <td>Robin Juhkental is the lead guitarist of Malco...</td>\n","      <td>D</td>\n","      <td>[[1, 458, 269, 262, 1862, 985, 265, 8175, 1433...</td>\n","      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n","      <td>[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Which of the following statements is true rega...</td>\n","      <td>The theory of relativity only encompasses one ...</td>\n","      <td>Special relativity explains the law of gravita...</td>\n","      <td>The theory of relativity does not encompass an...</td>\n","      <td>Special relativity applies to the cosmological...</td>\n","      <td>General relativity only applies to the motion ...</td>\n","      <td>D</td>\n","      <td>[[1, 2597, 265, 262, 776, 3741, 269, 980, 1712...</td>\n","      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n","      <td>[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>In which country was the 1920 collection of co...</td>\n","      <td>United States</td>\n","      <td>Germany</td>\n","      <td>Australia</td>\n","      <td>France</td>\n","      <td>England</td>\n","      <td>E</td>\n","      <td>[[1, 344, 319, 658, 284, 262, 8547, 1283, 265,...</td>\n","      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n","      <td>[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>What is one of the areas that Shimon Dovid Cow...</td>\n","      <td>Environmental conservation, opposing deforesta...</td>\n","      <td>Homosexuality, looser abortion laws and volunt...</td>\n","      <td>Freedom of speech, advocating for increased li...</td>\n","      <td>Gun control, advocating for stricter regulatio...</td>\n","      <td>Animal rights, opposing the use of animals for...</td>\n","      <td>B</td>\n","      <td>[[1, 458, 269, 311, 265, 262, 893, 272, 81398,...</td>\n","      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n","      <td>[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>When did the Dirt Road Diaries Tour begin and ...</td>\n","      <td>February 17, 2014 - November 26, 2014</td>\n","      <td>January 17, 2014 - October 26, 2014</td>\n","      <td>February 17, 2013 - November 26, 2013</td>\n","      <td>March 17, 2013 - December 26, 2013</td>\n","      <td>January 17, 2013 - October 26, 2013</td>\n","      <td>E</td>\n","      <td>[[1, 486, 464, 262, 27469, 2055, 40512, 4590, ...</td>\n","      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,...</td>\n","      <td>[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...</td>\n","      <td>4</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                              prompt  \\\n","0  What is the primary role of Robin Juhkental in...   \n","1  Which of the following statements is true rega...   \n","2  In which country was the 1920 collection of co...   \n","3  What is one of the areas that Shimon Dovid Cow...   \n","4  When did the Dirt Road Diaries Tour begin and ...   \n","\n","                                                   A  \\\n","0  Robin Juhkental is the bassist of Malcolm Linc...   \n","1  The theory of relativity only encompasses one ...   \n","2                                      United States   \n","3  Environmental conservation, opposing deforesta...   \n","4              February 17, 2014 - November 26, 2014   \n","\n","                                                   B  \\\n","0  Robin Juhkental is the keyboardist of Malcolm ...   \n","1  Special relativity explains the law of gravita...   \n","2                                            Germany   \n","3  Homosexuality, looser abortion laws and volunt...   \n","4                January 17, 2014 - October 26, 2014   \n","\n","                                                   C  \\\n","0  Robin Juhkental is the drummer of Malcolm Linc...   \n","1  The theory of relativity does not encompass an...   \n","2                                          Australia   \n","3  Freedom of speech, advocating for increased li...   \n","4              February 17, 2013 - November 26, 2013   \n","\n","                                                   D  \\\n","0  Robin Juhkental is the lead singer of Malcolm ...   \n","1  Special relativity applies to the cosmological...   \n","2                                             France   \n","3  Gun control, advocating for stricter regulatio...   \n","4                 March 17, 2013 - December 26, 2013   \n","\n","                                                   E answer  \\\n","0  Robin Juhkental is the lead guitarist of Malco...      D   \n","1  General relativity only applies to the motion ...      D   \n","2                                            England      E   \n","3  Animal rights, opposing the use of animals for...      B   \n","4                January 17, 2013 - October 26, 2013      E   \n","\n","                                           input_ids  \\\n","0  [[1, 458, 269, 262, 1862, 985, 265, 8175, 1433...   \n","1  [[1, 2597, 265, 262, 776, 3741, 269, 980, 1712...   \n","2  [[1, 344, 319, 658, 284, 262, 8547, 1283, 265,...   \n","3  [[1, 458, 269, 311, 265, 262, 893, 272, 81398,...   \n","4  [[1, 486, 464, 262, 27469, 2055, 40512, 4590, ...   \n","\n","                                      token_type_ids  \\\n","0  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...   \n","1  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...   \n","2  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...   \n","3  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...   \n","4  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,...   \n","\n","                                      attention_mask  label  \n","0  [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...      3  \n","1  [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...      3  \n","2  [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...      4  \n","3  [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...      1  \n","4  [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...      4  "]},"execution_count":162,"metadata":{},"output_type":"execute_result"}],"source":["# 直接map会增加column数量\n","tokenized_dataset1 = dataset.map(preprocess)\n","tokenized_dataset_pandas1 = tokenized_dataset1.to_pandas()\n","\n","tokenized_dataset_pandas1.head()"]},{"cell_type":"markdown","metadata":{},"source":["And let us now preprocess the examples for training."]},{"cell_type":"code","execution_count":163,"metadata":{},"outputs":[{"data":{"text/plain":["dict_keys(['prompt', 'A', 'B', 'C', 'D', 'E', 'answer', 'input_ids', 'token_type_ids', 'attention_mask', 'label'])"]},"execution_count":163,"metadata":{},"output_type":"execute_result"}],"source":["tokenized_dataset1[0].keys()"]},{"cell_type":"code","execution_count":164,"metadata":{},"outputs":[{"data":{"text/plain":["dict"]},"execution_count":164,"metadata":{},"output_type":"execute_result"}],"source":["type(tokenized_dataset1[0])"]},{"cell_type":"code","execution_count":165,"metadata":{"execution":{"iopub.execute_input":"2023-07-21T12:18:13.780864Z","iopub.status.busy":"2023-07-21T12:18:13.78041Z","iopub.status.idle":"2023-07-21T12:18:14.684855Z","shell.execute_reply":"2023-07-21T12:18:14.683889Z","shell.execute_reply.started":"2023-07-21T12:18:13.780824Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Map: 100%|██████████| 26900/26900 [00:07<00:00, 3467.38 examples/s]\n"]},{"data":{"text/plain":["Dataset({\n","    features: ['input_ids', 'token_type_ids', 'attention_mask', 'label'],\n","    num_rows: 26900\n","})"]},"execution_count":165,"metadata":{},"output_type":"execute_result"}],"source":["# input_ids: Construct a DeBERTa tokenizer. Based on byte-level Byte-Pair-Encoding.\n","# token type_id:Create a mask from the two sequences passed to be used in a sequence-pair classification task. A DeBERTa\n","tokenized_dataset = dataset.map(preprocess, remove_columns=['prompt', 'A', 'B', 'C', 'D', 'E', 'answer'])\n","tokenized_dataset_pandas = tokenized_dataset.to_pandas()\n","tokenized_dataset"]},{"cell_type":"code","execution_count":166,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["What is the primary role of Robin Juhkental in the band Malcolm Lincoln?\n","Robin Juhkental is the bassist of Malcolm Lincoln and is responsible for laying down the foundation of their music.\n","Robin Juhkental is the keyboardist of Malcolm Lincoln and adds atmospheric sounds to their music.\n","Robin Juhkental is the drummer of Malcolm Lincoln and keeps the beat for the band's songs.\n","Robin Juhkental is the lead singer of Malcolm Lincoln and provides vocals for the band's songs.\n","Robin Juhkental is the lead guitarist of Malcolm Lincoln and is responsible for creating unique guitar melodies and solos.\n"]}],"source":["print(df_train.prompt[0])\n","print(df_train.A[0])\n","print(df_train.B[0])\n","print(df_train.C[0])\n","print(df_train.D[0])\n","print(df_train.E[0])"]},{"cell_type":"code","execution_count":167,"metadata":{},"outputs":[{"data":{"text/plain":["array([array([    1,   458,   269,   262,  1862,   985,   265,  8175, 14334,\n","              41672, 91707,   267,   262,  1704, 15084,  6175,   302,     2,\n","               8175, 14334, 41672, 91707,   269,   262, 26623,   265, 15084,\n","               6175,   263,   269,  1744,   270,  9022,   444,   262,  3332,\n","                265,   308,   755,   260,     2], dtype=int32)              ,\n","       array([    1,   458,   269,   262,  1862,   985,   265,  8175, 14334,\n","              41672, 91707,   267,   262,  1704, 15084,  6175,   302,     2,\n","               8175, 14334, 41672, 91707,   269,   262, 67509,   265, 15084,\n","               6175,   263,  3814, 13123,  2163,   264,   308,   755,   260,\n","                  2], dtype=int32)                                          ,\n","       array([    1,   458,   269,   262,  1862,   985,   265,  8175, 14334,\n","              41672, 91707,   267,   262,  1704, 15084,  6175,   302,     2,\n","               8175, 14334, 41672, 91707,   269,   262, 16090,   265, 15084,\n","               6175,   263,  3425,   262,  2584,   270,   262,  1704,   280,\n","                268,  2397,   260,     2], dtype=int32)                     ,\n","       array([    1,   458,   269,   262,  1862,   985,   265,  8175, 14334,\n","              41672, 91707,   267,   262,  1704, 15084,  6175,   302,     2,\n","               8175, 14334, 41672, 91707,   269,   262,   917,  4804,   265,\n","              15084,  6175,   263,   888, 10023,   270,   262,  1704,   280,\n","                268,  2397,   260,     2], dtype=int32)                     ,\n","       array([    1,   458,   269,   262,  1862,   985,   265,  8175, 14334,\n","              41672, 91707,   267,   262,  1704, 15084,  6175,   302,     2,\n","               8175, 14334, 41672, 91707,   269,   262,   917, 14330,   265,\n","              15084,  6175,   263,   269,  1744,   270,  1512,   988,  4066,\n","              21052,   263, 39558,   260,     2], dtype=int32)              ],\n","      dtype=object)"]},"execution_count":167,"metadata":{},"output_type":"execute_result"}],"source":["# input_ids — List of token ids to be fed to a model.They are token indices, numerical representations of tokens building the sequences that will be used as input by the model.\n","tokenized_dataset_pandas.input_ids[0]"]},{"cell_type":"code","execution_count":170,"metadata":{},"outputs":[{"data":{"text/plain":["array([array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,\n","              1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n","             dtype=int8)                                                       ,\n","       array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,\n","              1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int8)        ,\n","       array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,\n","              1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int8),\n","       array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,\n","              1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int8),\n","       array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,\n","              1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n","             dtype=int8)                                                       ],\n","      dtype=object)"]},"execution_count":170,"metadata":{},"output_type":"execute_result"}],"source":["# List of token type ids to be fed to a model \n","'''\n","token\n","A part of a sentence, usually a word, but can also be a subword (non-common\n"," words are often split in subwords) or a punctuation symbol.\n","token Type IDs\n","Some models’ purpose is to do classification on pairs of sentences or question answering.\n","We can use our tokenizer to automatically generate such a sentence by passing the two\n"," sequences to tokenizer as two arguments (and not a list, like before) like this\n"," \n","from transformers import BertTokenizer\n","\n","tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n","sequence_a = \"HuggingFace is based in NYC\"\n","sequence_b = \"Where is HuggingFace based?\"\n","\n","encoded_dict = tokenizer(sequence_a, sequence_b)\n","decoded = tokenizer.decode(encoded_dict[\"input_ids\"])\n","\n","which will return:\n","\n","print(decoded)\n","[CLS] HuggingFace is based in NYC [SEP] Where is HuggingFace based? [SEP]\n","\n","The first sequence, the “context” used for the question, \n","has all its tokens represented by a 0, whereas the second sequence, \n","corresponding to the “question”, has all its tokens represented by a 1.\n","'''\n","tokenized_dataset_pandas.token_type_ids[0]"]},{"cell_type":"code","execution_count":171,"metadata":{},"outputs":[{"data":{"text/plain":["array([array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","              1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n","             dtype=int8)                                                       ,\n","       array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","              1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int8)        ,\n","       array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","              1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int8),\n","       array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","              1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int8),\n","       array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","              1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n","             dtype=int8)                                                       ],\n","      dtype=object)"]},"execution_count":171,"metadata":{},"output_type":"execute_result"}],"source":["# The attention mask is a binary tensor indicating the position of t\n","# The padded indices so that the model does not attend to them. \n","# For the BertTokenizer , 1 indicates a value that should be attended to, while 0 indicates a padded value.\n","'''\n","tokenizer.padding_side = \"left\"\n","tokenizer.pad_token = tokenizer.eos_token\n","\n","sentences = [\"It will rain in the\",\n","            \"I want to eat a big bowl of\",\n","            \"My dog is\"]\n","inputs = tokenizer(sentences, return_tensors=\"pt\", padding=True)\n","\n","output_sequences = gpt2.generate(**inputs)\n","\n","for seq in output_sequences:\n","    print(tokenizer.decode(seq))\n","    \n","{'input_ids': tensor([\n","    [50256, 50256, 50256,  1026,   481,  6290,   287,   262],\n","    [   40,   765,   284,  4483,   257,  1263,  9396,   286],\n","    [50256, 50256, 50256, 50256, 50256,  3666,  3290,   318]\n","  ]),\n","'attention_mask': tensor([\n","    [0, 0, 0, 1, 1, 1, 1, 1],\n","    [1, 1, 1, 1, 1, 1, 1, 1],\n","    [0, 0, 0, 0, 0, 1, 1, 1]\n","  ])}\n","'''\n","tokenized_dataset_pandas.attention_mask[0]"]},{"cell_type":"code","execution_count":207,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[3]\n","1\n","5\n","[[{'input_ids': [1, 458, 269, 262, 1862, 985, 265, 8175, 14334, 41672, 91707, 267, 262, 1704, 15084, 6175, 302, 2, 8175, 14334, 41672, 91707, 269, 262, 26623, 265, 15084, 6175, 263, 269, 1744, 270, 9022, 444, 262, 3332, 265, 308, 755, 260, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, {'input_ids': [1, 458, 269, 262, 1862, 985, 265, 8175, 14334, 41672, 91707, 267, 262, 1704, 15084, 6175, 302, 2, 8175, 14334, 41672, 91707, 269, 262, 67509, 265, 15084, 6175, 263, 3814, 13123, 2163, 264, 308, 755, 260, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, {'input_ids': [1, 458, 269, 262, 1862, 985, 265, 8175, 14334, 41672, 91707, 267, 262, 1704, 15084, 6175, 302, 2, 8175, 14334, 41672, 91707, 269, 262, 16090, 265, 15084, 6175, 263, 3425, 262, 2584, 270, 262, 1704, 280, 268, 2397, 260, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, {'input_ids': [1, 458, 269, 262, 1862, 985, 265, 8175, 14334, 41672, 91707, 267, 262, 1704, 15084, 6175, 302, 2, 8175, 14334, 41672, 91707, 269, 262, 917, 4804, 265, 15084, 6175, 263, 888, 10023, 270, 262, 1704, 280, 268, 2397, 260, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, {'input_ids': [1, 458, 269, 262, 1862, 985, 265, 8175, 14334, 41672, 91707, 267, 262, 1704, 15084, 6175, 302, 2, 8175, 14334, 41672, 91707, 269, 262, 917, 14330, 265, 15084, 6175, 263, 269, 1744, 270, 1512, 988, 4066, 21052, 263, 39558, 260, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}]]\n","[{'input_ids': [1, 458, 269, 262, 1862, 985, 265, 8175, 14334, 41672, 91707, 267, 262, 1704, 15084, 6175, 302, 2, 8175, 14334, 41672, 91707, 269, 262, 26623, 265, 15084, 6175, 263, 269, 1744, 270, 9022, 444, 262, 3332, 265, 308, 755, 260, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, {'input_ids': [1, 458, 269, 262, 1862, 985, 265, 8175, 14334, 41672, 91707, 267, 262, 1704, 15084, 6175, 302, 2, 8175, 14334, 41672, 91707, 269, 262, 67509, 265, 15084, 6175, 263, 3814, 13123, 2163, 264, 308, 755, 260, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, {'input_ids': [1, 458, 269, 262, 1862, 985, 265, 8175, 14334, 41672, 91707, 267, 262, 1704, 15084, 6175, 302, 2, 8175, 14334, 41672, 91707, 269, 262, 16090, 265, 15084, 6175, 263, 3425, 262, 2584, 270, 262, 1704, 280, 268, 2397, 260, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, {'input_ids': [1, 458, 269, 262, 1862, 985, 265, 8175, 14334, 41672, 91707, 267, 262, 1704, 15084, 6175, 302, 2, 8175, 14334, 41672, 91707, 269, 262, 917, 4804, 265, 15084, 6175, 263, 888, 10023, 270, 262, 1704, 280, 268, 2397, 260, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, {'input_ids': [1, 458, 269, 262, 1862, 985, 265, 8175, 14334, 41672, 91707, 267, 262, 1704, 15084, 6175, 302, 2, 8175, 14334, 41672, 91707, 269, 262, 917, 14330, 265, 15084, 6175, 263, 269, 1744, 270, 1512, 988, 4066, 21052, 263, 39558, 260, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}]\n","tensor([[    1,   458,   269,   262,  1862,   985,   265,  8175, 14334, 41672,\n","         91707,   267,   262,  1704, 15084,  6175,   302,     2,  8175, 14334,\n","         41672, 91707,   269,   262, 26623,   265, 15084,  6175,   263,   269,\n","          1744,   270,  9022,   444,   262,  3332,   265,   308,   755,   260,\n","             2],\n","        [    1,   458,   269,   262,  1862,   985,   265,  8175, 14334, 41672,\n","         91707,   267,   262,  1704, 15084,  6175,   302,     2,  8175, 14334,\n","         41672, 91707,   269,   262, 67509,   265, 15084,  6175,   263,  3814,\n","         13123,  2163,   264,   308,   755,   260,     2,     0,     0,     0,\n","             0],\n","        [    1,   458,   269,   262,  1862,   985,   265,  8175, 14334, 41672,\n","         91707,   267,   262,  1704, 15084,  6175,   302,     2,  8175, 14334,\n","         41672, 91707,   269,   262, 16090,   265, 15084,  6175,   263,  3425,\n","           262,  2584,   270,   262,  1704,   280,   268,  2397,   260,     2,\n","             0],\n","        [    1,   458,   269,   262,  1862,   985,   265,  8175, 14334, 41672,\n","         91707,   267,   262,  1704, 15084,  6175,   302,     2,  8175, 14334,\n","         41672, 91707,   269,   262,   917,  4804,   265, 15084,  6175,   263,\n","           888, 10023,   270,   262,  1704,   280,   268,  2397,   260,     2,\n","             0],\n","        [    1,   458,   269,   262,  1862,   985,   265,  8175, 14334, 41672,\n","         91707,   267,   262,  1704, 15084,  6175,   302,     2,  8175, 14334,\n","         41672, 91707,   269,   262,   917, 14330,   265, 15084,  6175,   263,\n","           269,  1744,   270,  1512,   988,  4066, 21052,   263, 39558,   260,\n","             2]])\n"]}],"source":["features = [tokenized_dataset[0]]\n","label_name = 'label' if 'label' in features[0].keys() else 'labels'\n","labels = [feature.pop(label_name) for feature in features]\n","print(labels)\n","batch_size = len(features)\n","print(batch_size)\n","num_choices = len(features[0]['input_ids'])\n","print(num_choices)\n","flattened_features = [\n","        [{k: v[i] for k, v in feature.items()} for i in range(num_choices)] for feature in features\n","    ]\n","print(flattened_features)# 多了一个外括号要去掉\n","flattened_features = sum(flattened_features, [])# 多了一个外括号要去掉，并没有相加任何数\n","print(flattened_features)\n","# pad 0,且把相同的key的value值合并了\n","batch = tokenizer.pad(\n","    flattened_features,\n","    padding=True,\n","    max_length=None,\n","    pad_to_multiple_of=None,\n","    return_tensors='pt',# 它指定了填充后的数据应该以 PyTorch 张量的形式返回。\n",")\n","print(batch.input_ids)\n","batch = {k: v.view(batch_size, num_choices, -1) for k, v in batch.items()}\n","#print(batch)\n","batch['labels'] = torch.tensor(labels, dtype=torch.int64)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","tokenized_dataset_valid = dataset_valid.map(preprocess, remove_columns=['prompt', 'A', 'B', 'C', 'D', 'E', 'answer'])"]},{"cell_type":"markdown","metadata":{},"source":["# Training"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import numpy as np\n","def map_at_3(predictions, labels):\n","    map_sum = 0\n","    pred = np.argsort(-1*np.array(predictions),axis=1)[:,:3]\n","    for x,y in zip(pred,labels):\n","        z = [1/i if y==j else 0 for i,j in zip([1,2,3],x)]\n","        map_sum += np.sum(z)\n","    return map_sum / len(predictions)\n","\n","# Define your custom evaluation function\n","def compute_metrics(p):\n","    predictions = p.predictions.tolist()\n","    labels = p.label_ids.tolist()\n","    return {\"map@3\": map_at_3(predictions, labels)}\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-21T12:18:14.693881Z","iopub.status.busy":"2023-07-21T12:18:14.693512Z","iopub.status.idle":"2023-07-21T12:20:57.538832Z","shell.execute_reply":"2023-07-21T12:20:57.537531Z","shell.execute_reply.started":"2023-07-21T12:18:14.693846Z"},"trusted":true},"outputs":[],"source":["training_args = TrainingArguments(\n","    # 具体来说，学习率预热是指在训练的开始阶段，逐渐增加学习率的过程。通常，在训练初期\n","    # 模型的权重随机初始化，对数据的拟合不够好，此时较小的学习率可以帮助模型更快地收敛\n","    # 到一个相对合适的权重状态。然后，随着训练的进行，学习率逐渐增加，以允许模型在局部极小值之间跳跃，以寻找更好的全局最小值。\n","    warmup_ratio=0.8,\n","    learning_rate=5e-6,\n","    load_best_model_at_end=True,\n","    evaluation_strategy='steps',\n","    logging_steps=500,\n","    metric_for_best_model='map@3',# 用于选择最佳模型的评估指标。在训练过程中，会监视此指标的变化，并选择在验证集上具有最佳该指标值的模型。\n","    per_device_train_batch_size=1, # 这是指每个GPU设备上的训练batch size大小。如果你有多个GPU，你可以选择同时在每个GPU上处理多个样本，以加速训练。这个参数会决定每个GPU上的批次大小。\n","    eval_steps=500,\n","    per_device_eval_batch_size=1,\n","    num_train_epochs=3,\n","    #report_to='none',\n","    output_dir='.',\n","    save_total_limit = 1,\n",")\n","\n","model = AutoModelForMultipleChoice.from_pretrained(deberta_v3_large)\n","'''\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    tokenizer=tokenizer,\n","    data_collator=DataCollatorForMultipleChoice(tokenizer=tokenizer),\n","    train_dataset=tokenized_dataset,\n","    compute_metrics = compute_metrics,\n",")\n","'''\n","# hugging face API for training using pytorch\n","# Hugging Face Transformers库中的 Trainer 对象，用于训练和评估NLP模型\n","trainer = Trainer(\n","    model=model,# can be transformer pretrain model\n","    args=training_args,# 这是一个包含训练参数的配置对象，其中包括诸如训练步骤数、学习率、批处理大小等训练超参数的设置。\n","    tokenizer=tokenizer,# 这是用于分词文本的分词器，它会将输入数据分词成模型可接受的形式。\n","    data_collator=DataCollatorForMultipleChoice(tokenizer=tokenizer),# Batch encoding, 专门用于处理多选题类型的数据集,这是一个数据收集器，用于将分词后的数据整理成适合模型输入的形式。因为此时tokenizedd后 数据都不等长，得padding才行\n","    train_dataset=tokenized_dataset,# 这是训练数据集，通常包含了经过分词和预处理后的数据。\n","    eval_dataset=tokenized_dataset_valid, # 这是用于模型验证的数据集，通常也包含了经过分词和预处理后的数据。\n","    compute_metrics = compute_metrics,# 这是一个函数，用于计算评估指标。你可以自定义此函数，以根据你的任务计算适当的指标，例如准确度、F1分数等。\n",")\n","\n","trainer.train()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["# Predicting on the test set"]},{"cell_type":"markdown","metadata":{},"source":["Now that we have trained our model, let us predict on the test set."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["test_df = pd.read_csv('./kaggle/input/train.csv')\n","test_df['answer'] = 'A' # dummy answer that allows us to preprocess the test dataset just like we preprocessed the train dataset\n","\n","tokenized_test_dataset = Dataset.from_pandas(test_df.drop(columns=['id'])).map(preprocess, remove_columns=['prompt', 'A', 'B', 'C', 'D', 'E'])"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["test_predictions = trainer.predict(tokenized_test_dataset).predictions\n","test_predictions[:4]"]},{"cell_type":"markdown","metadata":{},"source":["The predictions are values output from the last layer of our neural network.\n","\n","Let's obtain the predicted answer ids by sorting them from largest to the smallest."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["predictions_as_ids = np.argsort(-test_predictions, 1)\n","predictions_as_ids[:3]"]},{"cell_type":"markdown","metadata":{},"source":["Let us now assign a letter corresponding to each predicted id (0 -> 'A', 1 -> 'B', etc). "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["predictions_as_answer_letters = np.array(list('ABCDE'))[predictions_as_ids]\n","predictions_as_answer_letters[:3]"]},{"cell_type":"markdown","metadata":{},"source":["And let us now go from this representation to outputting a string with 3 highest rated answers seperated by a space."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["predictions_as_string = test_df['prediction'] = [\n","    ' '.join(row) for row in predictions_as_answer_letters[:, :3]\n","]\n","predictions_as_string[:3]"]},{"cell_type":"markdown","metadata":{},"source":["And we are done! 🥳\n","\n","Let us now output our submission."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["submission = test_df[['id', 'prediction']]\n","submission.to_csv('submission.csv', index=False)\n","\n","pd.read_csv('submission.csv').head()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["I hope you enjoyed this notebook!\n","\n","If you found it useful, please upvote 👉 [the corresponding dataset with 500 new training examples](https://www.kaggle.com/datasets/radek1/additional-train-data-for-llm-science-exam) 👈\n","\n","Thank you, appreciate your help! 🙏😊\n","\n","Thank you for reading and happy Kaggling!"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.17"}},"nbformat":4,"nbformat_minor":4}
